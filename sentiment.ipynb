{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from plotly import express as px\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from afinn import Afinn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('translated_tweets.pkl')\n",
    "additional_stopwords = ['na', 'https', 'ha', 'hahaha', 'haha', 'lang', 'ang', 'yan', 'ng', 'sa', 'rin', 'yun', 'yang', 'si', 'ako', 'siya', 'ka', 'po', \n",
    "                        'mga', 'yung', 'pa', 'pala', 'na', 'ni', 'sya', 'ba', 'ko', 'nyo', 'man']\n",
    "additional_stopwords = set(list(ENGLISH_STOP_WORDS) + additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn(emoticons=True)\n",
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['afinn'] = df['translated_text'].apply(lambda x: x['translatedText']).apply(lambda x: afinn.score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader'] = df['translated_text'].apply(lambda x: x['translatedText']).apply(lambda x: vader_analyzer.polarity_scores(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['translated_text'] = df['translated_text'].apply(lambda x: x['translatedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NRC\n",
    "def text_emotion(df, column):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column of text and adds 10 columns to the\n",
    "    DataFrame for each of the 10 emotions in the NRC Emotion Lexicon, with each\n",
    "    column containing the value of the text in that emotions\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with ten new columns\n",
    "    '''\n",
    "\n",
    "    new_df = df.copy()\n",
    "\n",
    "    filepath = ('data/'\n",
    "                'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')\n",
    "    emolex_df = pd.read_csv(filepath,\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep='\\t')\n",
    "    emolex_words = emolex_df.pivot(index='word',\n",
    "                                   columns='emotion',\n",
    "                                   values='association').reset_index()\n",
    "    emotions = emolex_words.columns.drop('word')\n",
    "    emo_df = pd.DataFrame(0, index=df.index, columns=emotions)\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    \n",
    "    book = ''\n",
    "    chapter = ''\n",
    "    \n",
    "    with tqdm(total=len(list(new_df.iterrows()))) as pbar:\n",
    "        for i, row in new_df.iterrows():\n",
    "            pbar.update(1)\n",
    "            document = word_tokenize(new_df.loc[i][column])\n",
    "            for word in document:\n",
    "                word = stemmer.stem(word.lower())\n",
    "                emo_score = emolex_words[emolex_words.word == word]\n",
    "                if not emo_score.empty:\n",
    "                    for emotion in list(emotions):\n",
    "                        emo_df.at[i, emotion] += emo_score[emotion]\n",
    "\n",
    "    new_df = pd.concat([new_df, emo_df], axis=1)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñà‚ñà‚ñà‚ñç                                                                          | 655/14923 [01:15<27:23,  8.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-257d33398abc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnrc_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_emotion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'translated_text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-99cea3bd4779>\u001b[0m in \u001b[0;36mtext_emotion\u001b[1;34m(df, column)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0memo_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memolex_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0memolex_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0memo_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0memotion\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\twitter-vaccine\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2969\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2973\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\twitter-vaccine\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nrc_df = text_emotion(df, 'translated_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at_date'] = pd.to_datetime(df['created_at_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader_neg'] = df['vader'].apply(lambda x: x['neg'])\n",
    "df['vader_neu'] = df['vader'].apply(lambda x: x['neu'])\n",
    "df['vader_pos'] = df['vader'].apply(lambda x: x['pos'])\n",
    "df['vader_compound'] = df['vader'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['afinn'].sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Vaccine Keyword - AFINN per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/AFINN_daily_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['afinn'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Vaccine Keyword - AFINN per month'\n",
    ")\n",
    "with open('plots/sentiment/AFINN_monthly_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['afinn'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Vaccine Keyword - AFINN per year'\n",
    ")\n",
    "with open('plots/sentiment/AFINN_yearly_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['afinn'].sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Dengue Keyword - AFINN per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/AFINN_daily_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['afinn'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Dengue Keyword - AFINN per month'\n",
    ")\n",
    "with open('plots/sentiment/AFINN_monthly_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['afinn'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Dengue Keyword - AFINN per year'\n",
    ")\n",
    "with open('plots/sentiment/AFINN_yearly_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['afinn'].sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Dengvaxia Keyword - AFINN per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/AFINN_daily_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['afinn'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Dengvaxia Keyword - AFINN per month'\n",
    ")\n",
    "with open('plots/sentiment/AFINN_monthly_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['afinn'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='afinn', title='Tweets that contain the Dengvaxia Keyword - AFINN per year'\n",
    ")\n",
    "with open('plots/sentiment/AFINN_yearly_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['vader_compound'].sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Vaccine Keyword - VADER per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/VADER_daily_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['vader_compound'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Vaccine Keyword - VADER per month'\n",
    ")\n",
    "with open('plots/sentiment/VADER_monthly_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['vader_compound'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Vaccine Keyword - VADER per year'\n",
    ")\n",
    "with open('plots/sentiment/VADER_yearly_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "    \n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['vader_compound'].sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Dengue Keyword - VADER per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/VADER_daily_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['vader_compound'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Dengue Keyword - VADER per month'\n",
    ")\n",
    "with open('plots/sentiment/VADER_monthly_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['vader_compound'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Dengue Keyword - VADER per year'\n",
    ")\n",
    "with open('plots/sentiment/VADER_yearly_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "    \n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['vader_compound'].sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Dengvaxia Keyword - VADER per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/VADER_daily_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['vader_compound'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Dengvaxia Keyword - VADER per month'\n",
    ")\n",
    "with open('plots/sentiment/VADER_monthly_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['vader_compound'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='vader_compound', title='Tweets that contain the Dengvaxia Keyword - VADER per year'\n",
    ")\n",
    "with open('plots/sentiment/VADER_yearly_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_cols = {col: 'nrc_' + col for col in ['anger',\n",
    "       'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive',\n",
    "       'sadness', 'surprise', 'trust']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    nrc_df.rename(nrc_cols, axis=1)[nrc_cols.values()],\n",
    "    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nrc_sum'] = df['nrc_positive'] - df['nrc_negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['nrc_sum'].sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title='Tweets that contain the Vaccine Keyword - NRC per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/NRC_daily_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['nrc_sum'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title='Tweets that contain the Vaccine Keyword - NRC per month'\n",
    ")\n",
    "with open('plots/sentiment/NRC_monthly_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('vacc')].groupby(['created_at_date'])['nrc_sum'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title='Tweets that contain the Vaccine Keyword - NRC per year'\n",
    ")\n",
    "with open('plots/sentiment/NRC_yearly_Tweets that contain the Vaccine Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "    \n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['nrc_sum'].sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title=f'Tweets that contain the Dengue Keyword - NRC {emotion} per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/NRC_daily_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['nrc_sum'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title='Tweets that contain the Dengue Keyword - NRC per month'\n",
    ")\n",
    "with open('plots/sentiment/NRC_monthly_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengue')].groupby(['created_at_date'])['nrc_sum'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title='Tweets that contain the Dengue Keyword - NRC per year'\n",
    ")\n",
    "with open('plots/sentiment/NRC_yearly_Tweets that contain the Dengue Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "    \n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['nrc_sum'].sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title='Tweets that contain the Dengvaxia Keyword - NRC per day'\n",
    ")\n",
    "\n",
    "with open('plots/sentiment/NRC_daily_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['nrc_sum'].sum().resample('M').sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title='Tweets that contain the Dengvaxia Keyword - NRC per month'\n",
    ")\n",
    "with open('plots/sentiment/NRC_monthly_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())\n",
    "\n",
    "\n",
    "tmp = px.line(\n",
    "    df[df['translated_text'].str.lower().str.contains('dengvax')].groupby(['created_at_date'])['nrc_sum'].sum().resample('Y').sum().reset_index(),\n",
    "    x='created_at_date', y='nrc_sum', title='Tweets that contain the Dengvaxia Keyword - NRC per year'\n",
    ")\n",
    "with open('plots/sentiment/NRC_yearly_Tweets that contain the Dengvaxia Keyword.html', 'w') as f:\n",
    "    f.write(tmp.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.drop([\n",
    "    'contributors', 'coordinates', 'entities','geo', 'id_str',\n",
    "       'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
    "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
    "       'in_reply_to_user_id_str', 'lang', 'matching_rules',\n",
    "       'place', 'source', 'text', 'truncated','display_text_range', 'extended_entities', 'extended_tweet',\n",
    "    'quoted_status_id_str', 'vader',\n",
    "       'quoted_status_permalink', \n",
    "], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['user_id'] = tmp['user'].apply(lambda x: x['id'])\n",
    "tmp['user_name'] = tmp['user'].apply(lambda x: x['screen_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\.conda\\envs\\twitter-vaccine\\lib\\site-packages\\xlsxwriter\\worksheet.py:915: UserWarning:\n",
      "\n",
      "Ignoring URL 'https://t.co/ZTVKFGmKYA%20SHOW%20na%20naman%20si%20Sen.%20Dick%20Gordon.%20No.1%20pa%20naman%20ikaw%20sa%20senator%20list%20ko%20kaso%20sa%20napapanood%20ko%20ngayon%20sa%20dengvaxia%20hearing%20nag.eexplain%20pa%20ung%20mga%20ininvite%20na%20resource%20person%20eh%20binabara%20mo%20na%20agad%20kasi%20ayaw%20mo%20nung%20sinasabi%20mapa-Pro%20or%20anti%20admin%20man.' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "\n",
      "C:\\Users\\PC\\.conda\\envs\\twitter-vaccine\\lib\\site-packages\\xlsxwriter\\worksheet.py:915: UserWarning:\n",
      "\n",
      "Ignoring URL 'https://t.co/lcG5aoJZ8S\n",
      "%22Acosta‚Äôs%20allegations%20have%20resulted¬†in%20the%20decline%20in%20vaccine%20confidence%20&amp;%20a%20rise%20in%20cases%20of%20Measles%20&amp;%20other%20vaccine%20preventable%20diseases.\n",
      "DOH%20noted%20that%20cases%20of%20measles%20&amp;%20rubella%20dramatically¬†increased%20to%20over%2018,000%20in%202018%20vs%203,804%20cases%20in%202017.%22%20üò†' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "\n",
      "C:\\Users\\PC\\.conda\\envs\\twitter-vaccine\\lib\\site-packages\\xlsxwriter\\worksheet.py:915: UserWarning:\n",
      "\n",
      "Ignoring URL 'https://t.co/lcG5aoJZ8S%20&quot;Acostas%20allegations%20have%20resulted%20in%20the%20decline%20in%20vaccine%20confidence%20&amp;%20a%20rise%20in%20cases%20of%20Measles%20&amp;%20other%20vaccine%20preventable%20diseases.%20DOH%20noted%20that%20cases%20of%20measles%20&amp;%20rubella%20dramaticallyincreased%20to%20over%2018,000%20in%202018%20vs%203,804%20cases%20in%202017.%20&quot;' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp['created_at'] = tmp['created_at'].astype(str)\n",
    "\n",
    "tmp.drop('user', axis=1).to_excel('tweets_with_sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_excel('tweets_with_sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['keyword_vaccine'] = tmp['translated_text'].str.lower().str.contains('vacc') | tmp['translated_text'].str.lower().str.contains('bakun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['keyword_dengvaxia'] = tmp['translated_text'].str.lower().str.contains('dengvax') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['keyword_dengue'] = tmp['translated_text'].str.lower().str.contains('dengue') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_excel('tweets_with_sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NRC for Tweets with Vaccine keyword per Month.html', 'w') as f:\n",
    "    t = px.line(tmp[tmp['keyword_vaccine']].groupby(['created_at_date'])[['nrc_anger', 'nrc_anticipation', 'nrc_disgust',\n",
    "       'nrc_fear', 'nrc_joy', 'nrc_negative', 'nrc_positive', 'nrc_sadness',\n",
    "       'nrc_surprise', 'nrc_trust']].sum().resample('M').sum().reset_index().melt('created_at_date'),\n",
    "        x='created_at_date', y='value', color='variable',\n",
    "        title='NRC for Tweets with Vaccine keyword per Month')\n",
    "    f.write(t.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NRC for Tweets with Dengue keyword per Month.html', 'w') as f:\n",
    "    t = px.line(tmp[tmp['keyword_dengue'].fillna(False)].groupby(['created_at_date'])[['nrc_anger', 'nrc_anticipation', 'nrc_disgust',\n",
    "       'nrc_fear', 'nrc_joy', 'nrc_negative', 'nrc_positive', 'nrc_sadness',\n",
    "       'nrc_surprise', 'nrc_trust']].sum().resample('M').sum().reset_index().melt('created_at_date'),\n",
    "        x='created_at_date', y='value', color='variable',\n",
    "        title='NRC for Tweets with Dengue keyword per Month')\n",
    "    f.write(t.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NRC for Tweets with Dengvaxia keyword per Month.html', 'w') as f:\n",
    "    t = px.line(tmp[tmp['keyword_dengvaxia'].fillna(False)].groupby(['created_at_date'])[['nrc_anger', 'nrc_anticipation', 'nrc_disgust',\n",
    "       'nrc_fear', 'nrc_joy', 'nrc_negative', 'nrc_positive', 'nrc_sadness',\n",
    "       'nrc_surprise', 'nrc_trust']].sum().resample('M').sum().reset_index().melt('created_at_date'),\n",
    "        x='created_at_date', y='value', color='variable',\n",
    "        title='NRC for Tweets with Dengvaxia keyword per Month')\n",
    "    f.write(t.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
